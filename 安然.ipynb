{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络数据分析-数据挖掘实验报告\n",
    "## 安然邮件通联关系提取\n",
    "#### *王凌           2019/5/18*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、实验目的和要求\n",
    "        给出安然公司的邮件数据集，获取邮件的收发关系，发送时间、正文内容等通联关系数据。利用Pagerank算法计算通联关系中每一个人物的权重，结合网络分析软件（Gephi）建立起通联网络，进行可视化。\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验流程\n",
    "1. 数据格式分析\n",
    "2. 通联关系网络——点的数据获取和构建\n",
    "3. 通联关系网络——边的数据获取和构建\n",
    "4. 一张等权重的无向通联关系图\n",
    "5. 基于Pagerank算法的有向图建立\n",
    "6. 人以群分——社团发现\n",
    "7. 总结\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1数据格式分析\n",
    "        首先我们打开一份邮件来查看一下邮件的格式。\n",
    "![邮件格式](./pics/1.png \"可选标题\")\n",
    "\n",
    "        在图片中，第一个绿色框内有一部分邮件头数据，包括收发件人的邮箱、邮件发送时间、主题等信息。第二个绿色框中是正文的信息。为了构建通联关系网络，我们需要的是发件人的信息和收件人的信息。这一部分信息在每封邮件中存在于两处。\n",
    "        一处在第一个绿色框中，呈现形式为发件人和收件人的邮箱地址。\n",
    "        另一处在黄色框中，呈现形式为收件人和发件人的姓名。\n",
    "        我采用黄色框中的数据作为构建通联关系的数据来源，不采用邮箱地址的原因很简单，看下面这个安然公司人员名单。\n",
    "![人员名单](./pics/2.png\"人员名单\")\n",
    "        \n",
    "        我们要建立的是安然高层管理人员的通联关系网络，高管人员的姓名在这个文档里都已经明确写出。结合高管人员姓名的格式和黄色框中人物姓名的格式，显然采用黄色框中的姓名比起绿色框的邮箱地址更具有优势。\n",
    "        那么实际上，从一封邮件中，我们已经可以获取到点和边的关系了，只要获取到这些数据，就可以构建出网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2通联关系网络——点的数据获取和构建\n",
    "    那么我们首先从人员名单中获取这群高管的姓名吧～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Meyers\n",
      "Thomas Martin\n",
      "Andrea Ring\n",
      "Andrew Lewis\n",
      "Andy Zipper\n",
      "Jeffrey Shankman\n",
      "Barry Tycholiz\n",
      "Benjamin Rogers\n",
      "Bill Rapp\n",
      "xxx\n",
      "\n",
      "Bradley Mckay\n",
      "xxx\n",
      "\n",
      "Richard Sanders\n",
      "Cara Semperger\n",
      "Daron Giron\n",
      "Charles Weldon\n",
      "Chris Dorland\n",
      "Chris Germany\n",
      "xxx\n",
      "\n",
      "Cooper Richey\n",
      "Craig Dean\n",
      "Dana Davis\n",
      "Dan Hyvl\n",
      "Danny McCarty\n",
      "Daren Farmer\n",
      "Darrell Schoolcraft\n",
      "Daron Giron\n",
      "David Delainey\n",
      "Susan Bailey\n",
      "xxx\n",
      "\n",
      "Diana Scholtes\n",
      "Thomas Martin\n",
      "Don Baughman\n",
      "Drew Fossum\n",
      "James Steffes\n",
      "xxx\n",
      "\n",
      "xxx\n",
      "\n",
      "Mark Haedicke\n",
      "Elizabeth Sager\n",
      "Eric Bass\n",
      "Eric Saibi\n",
      "Errol McLaughlin\n",
      "Mark Taylor\n",
      "Sandra Brawner\n",
      "Larry Campbell\n",
      "Peter Keavey\n",
      "Fletcher Sturm\n",
      "Frank Ermis\n",
      "Geir Solberg\n",
      "Geoffery Storey\n",
      "Gerald Nemec\n",
      "Greg Whalley\n",
      "xxx\n",
      "\n",
      "Harpreet Arora\n",
      "Andrew Lewis\n",
      "Holden Salisbury\n",
      "Hunter Shively\n",
      "James Derrick\n",
      "James Steffes\n",
      "Jane Tholt\n",
      "xxx\n",
      "\n",
      "Jason Wolfe\n",
      "Jay Reitmeyer\n",
      "Jeff Dasovich\n",
      "Jeff King\n",
      "John Hodge\n",
      "Jeffrey Shankman\n",
      "Jeffery Skilling\n",
      "Daren Farmer\n",
      "xxx\n",
      "\n",
      "Jim Schwieger\n",
      "Vince Kaminski\n",
      "Vince Kaminski\n",
      "Steven Kean\n",
      "xxx\n",
      "\n",
      "Joe Parks\n",
      "Joe Quenet\n",
      "Joe Stepenovitch\n",
      "John Arnold\n",
      "John Forney\n",
      "xxx\n",
      "\n",
      "John Hodge\n",
      "John Lavorato\n",
      "John Zufferli\n",
      "Jonathan Mckay\n",
      "Fletcher Sturm\n",
      "Juan Hernandez\n",
      "xxx\n",
      "\n",
      "Judy Townsend\n",
      "Philip Allen\n",
      "Kam Keiser\n",
      "Kate Symes\n",
      "Kay Mann\n",
      "Keith Holst\n",
      "Kenneth Lay\n",
      "Kevin Hyatt\n",
      "Kevin Presto\n",
      "Kevin Ruscitti\n",
      "Kimberly Watson\n",
      "Kim Ward\n",
      "Larry Campbell\n",
      "Lawrence May\n",
      "Randall Gay\n",
      "Lindy Donoho\n",
      "xxx\n",
      "\n",
      "xxx\n",
      "\n",
      "Patrice Mims\n",
      "Louise Kitchen\n",
      "Lynn Blair\n",
      "xxx\n",
      "\n",
      "Marie Heard\n",
      "Mark Haedicke\n",
      "Mark Haedicke\n",
      "xxx\n",
      "\n",
      "Mark Taylor\n",
      "Mark Whitt\n",
      "Martin Cuilla\n",
      "Mary Fischer\n",
      "Matthew Lenhart\n",
      "Matthew Motley\n",
      "xxx\n",
      "\n",
      "John Forney\n",
      "Michelle Lokay\n",
      "Michelle Cash\n",
      "Michelle Lokay\n",
      "Mike Carson\n",
      "Michael Grigsby\n",
      "Michael Maggi\n",
      "xxx\n",
      "\n",
      "Mike Swerzbin\n",
      "Phillip Love\n",
      "Monika Causholli\n",
      "xxx\n",
      "\n",
      "Kevin Presto\n",
      "Susan Scott\n",
      "xxx\n",
      "\n",
      "Jane Tholt\n",
      "Patrice Mims\n",
      "Paul Thomas\n",
      "Peter Keavey\n",
      "Philip Allen\n",
      "Phillip Love\n",
      "Phillip Platter\n",
      "Randall Gay\n",
      "Richard Ring\n",
      "Richard Sanders\n",
      "Richard Shapiro\n",
      "Rick Buy\n",
      "Robert Badeer\n",
      "Robert Benson\n",
      "xxx\n",
      "\n",
      "Rod Hayslett\n",
      "Ryan Slinger\n",
      "Sally Beck\n",
      "Sandra Brawner\n",
      "xxx\n",
      "\n",
      "xxx\n",
      "\n",
      "Scott Neal\n",
      "Shelley Corman\n",
      "Hunter Shively\n",
      "Stacy Dickson\n",
      "Stanley Horton\n",
      "Stephanie Panus\n",
      "Steven Kean\n",
      "xxx\n",
      "\n",
      "Susan Bailey\n",
      "Susan Pereira\n",
      "Susan Scott\n",
      "Kim Ward\n",
      "Tana Jones\n",
      "Teb Lokey\n",
      "Theresa Staab\n",
      "John Hodge\n",
      "Thomas Martin\n",
      "Paul Lucci\n",
      "Tom Donohoe\n",
      "Tori Kuykendall\n",
      "Tracy Geaccone\n",
      "Vince Kaminski\n",
      "Vladi Pimenov\n",
      "Charles Weldon\n",
      "David Delainey\n",
      "Susan Pereira\n",
      "Stacey White\n"
     ]
    }
   ],
   "source": [
    "#读取安然高层人员信息文档\n",
    "#input 人员信息\n",
    "#output 人员姓名\n",
    "path = './enron employees.txt'\n",
    "with open(path, 'r') as f:\n",
    "    for each in f.readlines():\n",
    "        p = each.split('\\t')\n",
    "        q = p[1].split('  ')\n",
    "        print(q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        可以看到除了正确获取到的人员姓名外，还存在两种类型的错误：\n",
    "        1、第一类是获取到“xxx”\n",
    "        2、第二类是获取到 空行\n",
    "        不过好在这类错误比例很低，手动删除即可。\n",
    "        之后将所有的人员姓名按照字母顺序排序之后保存到一个csv中，命名为“new_member.csv”。\n",
    "        csv格式示例如下：\n",
    "|  id|\n",
    "|:-:|\n",
    "|Andrea Ring|\n",
    "|Andrew Lewis|\n",
    "|...|\n",
    "|Vladi Pimenov|\n",
    "        \n",
    "        得到这些姓名之后，通联关系网络的“点”就已经得到了。\n",
    "        下一步我们要获取所有人员的\n",
    "\n",
    "<div align=\"center\"> 姓 和 名 </div>\n",
    "        \n",
    "        这一步的意义在哪里呢？后面再说，先看结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mike', 'Eric', 'Vince', 'Jim', 'Kate', 'Andrea', 'Teb', 'Don', 'John', 'Philip', 'Patrice', 'Jeffery', 'Richard', 'Tana', 'Rick', 'Joe', 'Chris', 'Robert', 'Kam', 'Gerald', 'John', 'Randall', 'Andrew', 'Keith', 'Rod', 'Matthew', 'Susan', 'id\\n', 'Danny', 'Barry', 'Phillip', 'Cooper', 'Jane', 'Eric', 'Greg', 'Susan', 'Richard', 'Darrell', 'Jay', 'Greg', 'Elizabeth', 'Michael', 'Susan', 'Stephanie', 'Kenneth', 'Scott', 'Chris', 'Frank', 'Kevin', 'Robert', 'Juan', 'Mike', 'Paul', 'Kevin', 'Joe', 'Steven', 'Jason', 'Jeff', 'Holden', 'Stanley', 'Martin', 'Shelley', 'David', 'James', 'Paul', 'Jeffrey', 'Marie', 'Theresa', 'Andy', 'Sandra', 'Errol', 'Hunter', 'Michelle', 'Tom', 'Fletcher', 'Dan', 'Kevin', 'Geir', 'Kim', 'Drew', 'Mark', 'Michael', 'Daren', 'John', 'Bill', 'Mark', 'Michelle', 'Thomas', 'Jeff', 'Joe', 'John', 'Lynn', 'Richard', 'Peter', 'Vladi', 'Lawrence', 'James', 'Dana', 'Sally', 'Diana', 'Phillip', 'Louise', 'Benjamin', 'Tori', 'Tracy', 'John', 'Judy', 'Kimberly', 'Larry', 'Lindy', 'Monika', 'Kay', 'Cara', 'Stacy', 'Mark', 'Ryan']\n",
      "['Carson', 'Saibi', 'Kaminski', 'Schwieger', 'Symes', 'Ring', 'Lokey', 'Baughman', 'Lavorato', 'Allen', 'Mims', 'Skilling', 'Sanders', 'Jones', 'Buy', 'Stepenovitch', 'Dorland', 'Benson', 'Keiser', 'Nemec', 'Hodge', 'Gay', 'Lewis', 'Holst', 'Hayslett', 'Lenhart', 'Pereira', 'id', 'McCarty', 'Tycholiz', 'Love', 'Richey', 'Tholt', 'Bass', 'Whalley', 'Bailey', 'Shapiro', 'Schoolcraft', 'Reitmeyer', 'Wolfe', 'Sager', 'Grigsby', 'Scott', 'Panus', 'Lay', 'Neal', 'Germany', 'Ermis', 'Presto', 'Badeer', 'Hernandez', 'Swerzbin', 'Thomas', 'Hyatt', 'Parks', 'Kean', 'Wolfe', 'Dasovich', 'Salisbury', 'Horton', 'Cuilla', 'Corman', 'Delainey', 'Steffes', 'Lucci', 'Shankman', 'Heard', 'Staab', 'Zipper', 'Brawner', 'McLaughlin', 'Shively', 'Lokay', 'Donohoe', 'Sturm', 'Hyvl', 'Ruscitti', 'Solberg', 'Ward', 'Fossum', 'Haedicke', 'Maggi', 'Farmer', 'Forney', 'Rapp', 'Taylor', 'Cash', 'Martin', 'King', 'Quenet', 'Arnold', 'Blair', 'Ring', 'Keavey', 'Pimenov', 'May', 'Derrick', 'Davis', 'Beck', 'Scholtes', 'Platter', 'Kitchen', 'Rogers', 'Kuykendall', 'Geaccone', 'Zufferli', 'Townsend', 'Watson', 'Campbell', 'Donoho', 'Causholli', 'Mann', 'Semperger', 'Dickson', 'Whitt', 'Slinger']\n"
     ]
    }
   ],
   "source": [
    "with open('./new_member.csv', 'r') as f:\n",
    "    p = f.readlines()\n",
    "    p = set(p)\n",
    "    p = list(p)\n",
    "    dd = []\n",
    "    ddd = []\n",
    "    for each in p:\n",
    "        x = each.split(' ')\n",
    "        dd.append(x[-1][:-1])\n",
    "        ddd.append(x[0])\n",
    "    print(ddd)#名\n",
    "    print(dd)#姓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3通联关系网络——边的数据获取和构建\n",
    "    对于图中的一条边，我们需要哪些信息？\n",
    "| 源 | 目标 | 权值 |\n",
    "| :----:| :----: | :----: |\n",
    "| 发件人 | 收件人 | 发件次数 |\n",
    "    \n",
    "    例如，在我们的整个邮件数据集中，A一共给B发送了k次邮件，那么就可以构建出这样的一条边的关系。\n",
    "    A, B, k\n",
    "    当然这是有向图的构建方式，对于无向图，可以采用一种冗余的构建方法。同样的条件A给B发送了k次邮件，那么我们得到的边的关系应该是：\n",
    "    A, B, k;\n",
    "    B, A, k;\n",
    "\n",
    "    具体的流程介绍如下：\n",
    "    首先是一个遍历文件和文件夹的函数，给定所有邮件的路径，就可以递归获取到所有邮件的路径。\n",
    "    之前教员要求的是获取all_documents文件夹中的所有邮件，但在实际操作中我发现，仅仅获取这一个文件夹中的邮件是显然不够的。许多人员的大量邮件分布在其它的文件夹中，因此遍历所有的文件夹才是合理的。\n",
    "---\n",
    "```python\n",
    "def file_path(filepath):\n",
    "    fileNames = os.listdir(filepath)  # 获取当前路径下的文件名，返回List\n",
    "    for file in fileNames:\n",
    "        newDir = filepath + '/' + file # 将文件命加入到当前文件路径后面\n",
    "        if os.path.isfile(newDir):  # 如果是文件\n",
    "            _from, _to, _cc, _bcc = find_from_to_cc(newDir)\n",
    "            summary_from_one(_from, _to, _cc, _bcc)\n",
    "        else:\n",
    "            file_path(newDir)                #如果不是文件，递归这个文件夹的路径\n",
    "```\n",
    "---\n",
    "    \n",
    "    可以看到，在file_path()这个函数中，还另外引用了find_from_to_cc()这个函数和summary_from_one()这个函数。先给出这两个函数的代码。\n",
    "---\n",
    "    \n",
    "```python\n",
    "def find_from_to_cc(file):\n",
    "    '''\n",
    "    这个函数的作用，就是对于单封邮件，获取邮件的发件人、收件人、抄送等信息，并保存到四个列表中，返回\n",
    "    '''\n",
    "    __from = []\n",
    "    __to = []\n",
    "    __cc = []\n",
    "    __bcc = []\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file)\n",
    "        #解决macOS的隐藏文件.DS_Store问题\n",
    "        if '.DS_Store' in file:\n",
    "            return __from, __to, __cc, __bcc\n",
    "        print(file)\n",
    "        try:\n",
    "            '''\n",
    "            最基本的正则表达式匹配邮件的通联关系（出现的人员信息）\n",
    "            有时候，一封邮件会发给多个人，在邮件中用逗号隔开：\n",
    "            X-To: A, B\n",
    "            那么这种情况，就需要对多个人进行分隔处理\n",
    "            '''\n",
    "            a = str(f.read())\n",
    "            f.close()\n",
    "            _from = re.findall(\"X-From: ([\\s\\S]*)\\nX-To\", a)\n",
    "            _to = re.findall(\"X-To: ([\\s\\S]*)\\nX-cc\", a)\n",
    "            _cc = re.findall(\"X-cc: ([\\s\\S]*)\\nX-bcc\", a)\n",
    "            _bcc = re.findall(\"X-bcc: ([\\s\\S]*)\\nX-Folder\", a)\n",
    "            #处理to cc bcc内多个人员的情况\n",
    "            _to_ = _to[0].split(', ')\n",
    "            _cc_ = _cc[0].split(', ')\n",
    "            _bcc_ = _bcc[0].split(', ')\n",
    "            '''\n",
    "            还记得一开始建立的姓和名两个列表吗？\n",
    "            邮件不仅仅是安然内部高层人士之间相互发送，邮件数据集中还存在大量冗余。\n",
    "            高层-普通职工\n",
    "            高层-外界人士\n",
    "            广告邮件\n",
    "            如何去除这些不需要的邮件？————根据收件人格式\n",
    "            '''\n",
    "            #将名称对应起来\n",
    "            #发件人的名称比较规范\n",
    "            from_name = _from[0].split(' ')\n",
    "            xing = from_name[-1]\n",
    "            ming = from_name[0]\n",
    "            if xing in m.xing and ming in m.ming:\n",
    "                __from.append(_from[0])\n",
    "            for each in _to_:\n",
    "                to_name = each.split(' ')\n",
    "                x = to_name[-1]\n",
    "                mm = to_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    __to.append(each)\n",
    "            for each in _cc_:\n",
    "                cc_name = each.split(' ')\n",
    "                x = cc_name[-1]\n",
    "                mm = cc_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    __cc.append(each)\n",
    "            for each in _bcc_:\n",
    "                bcc_name = each.split(' ')\n",
    "                x = bcc_name[-1]\n",
    "                mm = bcc_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    __bcc.append(each)\n",
    "        except:\n",
    "            None\n",
    "        return __from, __to, __cc, __bcc\n",
    "```\n",
    "---    \n",
    "    \n",
    "```python\n",
    "def summary_from_one(_from, _to, _cc, _bcc):\n",
    "    #print(_from,_to,_cc,_bcc)\n",
    "    '''\n",
    "    这里存在一个全局变量finald，是一个list，作用是保存下每一次邮件的通联关系\n",
    "    例如，遍历到某一封邮件，经由find_from_to_cc()函数后，返回四个列表。\n",
    "    _from = [A]\n",
    "    _to = [B, C]\n",
    "    _cc = [D, E]\n",
    "    _bcc = [F, G]\n",
    "    意思就是该邮件的发件人是A，发送给了B和C，抄送给了D, E, F, G\n",
    "    那么finald中就会append这些项:\n",
    "    (A, B), (A, C), (A, D), (A, E), (A, F)\n",
    "    '''\n",
    "    global finald\n",
    "    if _from != []:\n",
    "        if _to != []:\n",
    "            for each in _to:\n",
    "                finald.append((_from[0], each))\n",
    "        if _cc != []:\n",
    "            for each in _cc:\n",
    "                finald.append((_from[0], each))\n",
    "        if _bcc != []:\n",
    "            for each in _bcc:\n",
    "                finald.append((_from[0], each))\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于收件人格式和“姓”“名”的匹配原则\n",
    "\n",
    "    我们之前已经提到，所有的邮件的X-to、X-cc和X-bcc出现的人物中，只要是安然的高管（new_member.csv），那么一定是以姓名的形式呈现。\n",
    "    例如这位M.Maggi\n",
    "!['M.Maggi'](./pics/3.png)\n",
    "\n",
    "    他在邮件中出现的时候，是以怎样的形式呢？\n",
    "!['M.Maggi'](./pics/4.png)\n",
    "\n",
    "    这样一下就能看的很清楚，高管的名字，在邮件内都是可以明确直接找到的。\n",
    "    非高管的情况如何？\n",
    "    \n",
    "\n",
    "!['M.Maggi'](./pics/5.png)  \n",
    "\n",
    "    \n",
    "    往往是姓名-邮箱的格式，偶尔会见到只有姓名或是只有邮箱的情况。 \n",
    "    \n",
    "    知道了什么样子的人才是高管之后，我们就可以开始生成这些高管之间的通联关系了。这时候就需要借助之前已经生成的“姓”和“名”两个列表。\n",
    "    生成的思路如下：\n",
    "    1、利用find_from_to_cc()函数，提取出每一封邮件的from, to, cc, bcc保存为四个列表。\n",
    "    2、分别对四个列表进行如下操作：\n",
    "        2.1遍历列表中每一个元素p\n",
    "        2.2元素p用空格分隔，如果出现了邮箱地址，那么p[-1]一定为地址，这类元素直接舍弃\n",
    "        2.3如果元素p是一个人的姓名，根据国外名前姓后原则，检测p[0]是否在“名”列表中，p[-1]是否在“姓”列表中，如果有一项不在，则舍弃元素\n",
    "        补充说明：在邮件中出现了一类人名，例如Ling A Wang，但是在高管名单中呈现为Ling Wang，这类姓名也会被保留下来\n",
    "    3、将经过上述操作的四个列表作为summary_from_one()函数输入，将通联关系保存到finald列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这一部分代码需要运行很长时间，原因在于需要对所有的邮件进行遍历。\n",
    "这里为了节省时间，只采用很少的一部分邮件作为样例。\n",
    "'''\n",
    "import os\n",
    "import re\n",
    "import m\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path(filepath):\n",
    "    fileNames = os.listdir(filepath)  # 获取当前路径下的文件名，返回List\n",
    "    #print(fileNames)\n",
    "    for file in fileNames:\n",
    "        newDir = filepath + '/' + file # 将文件命加入到当前文件路径后面\n",
    "        # if os.path.isdir(newDir): # 如果是文件夹\n",
    "        if os.path.isfile(newDir):  # 如果是文件\n",
    "            #find_from_to_cc(newDir)\n",
    "            _from, _to, _cc, _bcc = find_from_to_cc(newDir)\n",
    "            summary_from_one(_from, _to, _cc, _bcc)\n",
    "        else:\n",
    "            file_path(newDir)                #如果不是文件，递归这个文件夹的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_from_one(_from, _to, _cc, _bcc):\n",
    "    #print(_from,_to,_cc,_bcc)\n",
    "    global finald\n",
    "    if _from != []:\n",
    "        if _to != []:\n",
    "            for each in _to:\n",
    "                finald.append((_from[0], each))\n",
    "        if _cc != []:\n",
    "            for each in _cc:\n",
    "                finald.append((_from[0], each))\n",
    "        if _bcc != []:\n",
    "            for each in _bcc:\n",
    "                finald.append((_from[0], each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_from_to_cc(file):\n",
    "    __from = []\n",
    "    __to = []\n",
    "    __cc = []\n",
    "    __bcc = []\n",
    "    with open(file, 'r') as f:\n",
    "        #print(file)\n",
    "        if '.DS_Store' in file:\n",
    "            return __from, __to, __cc, __bcc\n",
    "#         print(file)\n",
    "        try:\n",
    "            a = str(f.read())\n",
    "            f.close()\n",
    "            _from = re.findall(\"X-From: ([\\s\\S]*)\\nX-To\", a)\n",
    "            _to = re.findall(\"X-To: ([\\s\\S]*)\\nX-cc\", a)\n",
    "            _cc = re.findall(\"X-cc: ([\\s\\S]*)\\nX-bcc\", a)\n",
    "            _bcc = re.findall(\"X-bcc: ([\\s\\S]*)\\nX-Folder\", a)\n",
    "            #处理to cc bcc内多个人员的情况\n",
    "            _to_ = _to[0].split(', ')\n",
    "            _cc_ = _cc[0].split(', ')\n",
    "            _bcc_ = _bcc[0].split(', ')\n",
    "\n",
    "            #将名称对应起来\n",
    "            #发件人的名称比较规范\n",
    "            from_name = _from[0].split(' ')\n",
    "            xing = from_name[-1]\n",
    "            ming = from_name[0]\n",
    "            if xing in m.xing and ming in m.ming:\n",
    "                #print(from_name)\n",
    "                __from.append(_from[0])\n",
    "                #print('from',_from)\n",
    "            for each in _to_:\n",
    "                to_name = each.split(' ')\n",
    "                x = to_name[-1]\n",
    "                mm = to_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    # print('------',each,'-------')\n",
    "                    __to.append(each)\n",
    "            for each in _cc_:\n",
    "                cc_name = each.split(' ')\n",
    "                x = cc_name[-1]\n",
    "                mm = cc_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    # print('------',each,'-------')\n",
    "                    __cc.append(each)\n",
    "            for each in _bcc_:\n",
    "                bcc_name = each.split(' ')\n",
    "                x = bcc_name[-1]\n",
    "                mm = bcc_name[0]\n",
    "                if x in m.xing and mm in m.ming:\n",
    "                    # print('------',each,'-------')\n",
    "                    __bcc.append(each)\n",
    "            # print('from', __from)\n",
    "            # print('to',__to)\n",
    "            # print('cc',__cc)\n",
    "            # print('bcc',__bcc)\n",
    "            #print(__from, __to, __cc, __bcc)\n",
    "        except:\n",
    "            None\n",
    "        return __from, __to, __cc, __bcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('Phillip K Allen', 'Keith Holst'): 156, ('Phillip K Allen', 'Mike Grigsby'): 131, ('Phillip K Allen', 'Frank Ermis'): 76, ('Phillip K Allen', 'John J Lavorato'): 63, ('Phillip K Allen', 'Matthew Lenhart'): 51, ('Phillip K Allen', 'Hunter S Shively'): 30, ('Phillip K Allen', 'Jay Reitmeyer'): 28, ('Phillip K Allen', 'Tori Kuykendall'): 24, ('Phillip K Allen', 'Jane M Tholt'): 20, ('Phillip K Allen', 'Robert Badeer'): 19, ('Phillip K Allen', 'Randall L Gay'): 16, ('Phillip K Allen', 'Cooper Richey'): 14, ('Phillip K Allen', 'James D Steffes'): 13, ('Phillip K Allen', 'Fletcher J Sturm'): 12, ('Phillip K Allen', 'Steven J Kean'): 9, ('Phillip K Allen', 'David W Delainey'): 8, ('Phillip K Allen', 'Paul T Lucci'): 8, ('Phillip K Allen', 'Barry Tycholiz'): 7, ('Phillip K Allen', 'Andy Zipper'): 7, ('Phillip K Allen', 'Susan M Scott'): 7, ('Phillip K Allen', 'Thomas A Martin'): 6, ('Phillip K Allen', 'Jeff Dasovich'): 5, ('Phillip K Allen', 'Richard B Sanders'): 5, ('Phillip K Allen', 'Richard Shapiro'): 5, ('Phillip K Allen', 'Sally Beck'): 4, ('Phillip K Allen', 'Mark Taylor'): 4, ('Phillip K Allen', 'Jeffrey T Hodge'): 4, ('Phillip K Allen', 'Frank L Davis'): 4, ('Kim Ward', 'Phillip K Allen'): 3, ('Phillip K Allen', 'Mark Scott'): 3, ('Richard Shapiro', 'Phillip K Allen'): 2, ('Richard Shapiro', 'Jane M Tholt'): 2, ('Phillip K Allen', 'Scott Neal'): 2, ('Phillip K Allen', 'John Arnold'): 2, ('Phillip K Allen', 'Eric Benson'): 2, ('Mark Whitt', 'Jay Reitmeyer'): 1, ('Mark Whitt', 'Mike Grigsby'): 1, ('Mark Whitt', 'Paul T Lucci'): 1, ('Mark Whitt', 'Phillip K Allen'): 1, ('Mark Whitt', 'Susan M Scott'): 1, ('Mark Whitt', 'Theresa Staab'): 1})\n"
     ]
    }
   ],
   "source": [
    "finald = []\n",
    "file_path('./maildir/allen-p')\n",
    "pp = Counter(finald)\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    如上面程序运行的结果展示的，allen-p这个人所有的邮件发送关系我们都已经掌握了。其中有部分邮件并不是由这个人所发送，可能是存在转存的原因，但是获取到的结果集合一定是我们需要的allen-p目标集合的一个超集。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来是一步非常重要的处理工作，用来修补上面收件人格式“姓”“名”匹配原则的漏洞\n",
    "    仔细思考，这个匹配原则是否存在漏洞？\n",
    "|  姓|名|\n",
    "|:-:|:-:|\n",
    "|王|凌|\n",
    "|吴|彦祖|\n",
    "|张|学友|\n",
    "|陈|奕迅|\n",
    "    \n",
    "    假设这四个人是公司的所有高管，已经构造出了“姓”“名”这两个列表，根据我们的匹配原则，如果出现了这么几个人\n",
    "|  姓|名|\n",
    "|:-:|:-:|\n",
    "|王|彦祖|\n",
    "|王|学友|\n",
    "|王|奕迅|\n",
    "\n",
    "    这两个人并不是公司的高管，但是也能够通过我们的匹配规则！为了解决这个问题，需要对发件人和高管名单再次进行一轮匹配，进一步剔除非高管人员。同时要注意，Phillip K Allen和Phillip Allen是同一个人，这种情况需要进行保留。\n",
    "    通过上面的操作，得到了一组正确无冗余的边的关系集合，将其保存到csv中。\n",
    "```python\n",
    "#无向图\n",
    "f = open('result.csv','a+')\n",
    "    for k,v in pp.items():\n",
    "        f.write(str(k[0])+','+str(k[1])+','+str(v)+'\\n')\n",
    "    f.close()\n",
    "#有向图\n",
    "f = open('result2.csv','a+')\n",
    "    for k,v in pp.items():\n",
    "        f.write(str(k[0])+','+str(k[1])+','+str(v)+'\\n')\n",
    "    f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    为了方便Gephi输入，给这份边的关系加上一个header。在这里，生成了两份csv，一份作为有向图（上）的边关系，一份作为无向图（下）的边关系。\n",
    "\n",
    "![有向图](./pics/6.png \"有向图\")\n",
    "\n",
    "---\n",
    "![无向图](./pics/7.png \"无向图\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4一张等权重的无向通联关系图\n",
    "\n",
    "    在构建出点（new_member.csv）和无向边(result.csv)这两个图的基本要素之后，就可以开始进行图的可视化。\n",
    "    输入点和无向边\n",
    "![点](./pics/8.png \"点\")\n",
    "![无向边](./pics/8.png \"无向边\")\n",
    "    \n",
    "    在Gephi内进行算法和参数调整，输入一个无向的通联关系图。\n",
    "![基础](./pics/基础.svg)\n",
    "\n",
    "---\n",
    "\n",
    "#### 如何评价这张无向图？\n",
    "* 评价一个节点的重要性\n",
    "    > 节点的大小\n",
    "    > 节点的度\n",
    "* 评价一条边的重要性\n",
    "    > 与节点相连接的边的权重\n",
    " \n",
    "---    \n",
    "        \n",
    "        对于一个节点，其大小在一定程度上反映了这个节点在网络中的重要性，这里节点label的大小和节点的重要性呈线性关系，因此，label越大，这个节点在网络中就越重要。\n",
    "        John J Lavorato，作为安然公司的CEO，在整个网络中占了最重要的地位。其次Louise Kitchen和David W Delainey，作为President和另一位CEO，在网络中也有着举足轻重的地位。\n",
    "        但是在网络中也有一些比较特殊的情况，例如同样身为CEO的Jeff Skilling，他所代表的节点并没有体现出非常明显的特征。实际上，这也是可以理解的，毕竟网络的构建来源于数据集，一方面，可能Jeff Skilling本身使用邮件的次数较少，因此留下的原始数据较少，另一方面，可能在提取通联关系的过程中又丢失了一部分数据。\n",
    "        对于边来说，在图中，可以明显看到存在若干条颜色较深、宽度较宽的边存在。这些边意味着两个节点之间的联系十分频繁。由于这是一张无向图，因此并不能够确定是双方相互来往，还是只有单方面发邮件较多。但是无论如何，这样的边始终意味着一种较为紧密的联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        利用Gephi的模块化功能，可以很方便地得到这样一个社团发现的结果。可以看到，在安然高层管理者中，总是用几位“大佬”真正负责着串联整个公司的作用，亦即这些节点是网络中十分关键的节点，各自位于一个社团的中央，和本社团其它节点、其它社团的节点都保持着频繁的联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5基于Pagerank算法的有向图建立\n",
    "        建立一张有向图，同样需要点和边的数据，在2.3中已经构建完成。下面就是利用Pagerank算法，对网络中的节点进行权值的度量。\n",
    "        首先读取节点和边的关系，构造转移矩阵。\n",
    "```python\n",
    "#首先读取点和边\n",
    "path = './result2的副本.csv'\n",
    "path_menber = './new_member.csv'\n",
    "member = pd.read_csv(path_menber)['id']\n",
    "#将人物姓名排序，方便对应后面的权值\n",
    "member = set(member)\n",
    "member = list(member)\n",
    "member.sort()\n",
    "n = len(member)\n",
    "#建立初始转移矩阵，初始化大小为n*n，全0\n",
    "target = np.zeros((n, n))\n",
    "target = np.array(target)\n",
    "route = pd.read_csv(path,header=None)\n",
    "route = np.array(route)\n",
    "'''\n",
    "route就是一组邮件的收发关系，包括三个元素(from, to, weight)\n",
    "根据转移矩阵的定义，根据收发人的index和weight调整转移矩阵相应元素的值\n",
    "'''\n",
    "for each in route:\n",
    "    from_ = each[0]\n",
    "    to_ = each[1]\n",
    "    weight = each[2]\n",
    "    k1 = member.index(from_)\n",
    "    k2 = member.index(to_)\n",
    "    target[k2][k1] = weight\n",
    "print(target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 4.00e+00 0.00e+00]\n",
      " ...\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 5.00e+00 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 4.97e+03 0.00e+00]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 0.00e+00 0.00e+00 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = './result2的副本.csv'\n",
    "path_menber = './new_member.csv'\n",
    "member = pd.read_csv(path_menber)['id']\n",
    "member = set(member)\n",
    "member = list(member)\n",
    "member.sort()\n",
    "n = len(member)\n",
    "target = np.zeros((n, n))\n",
    "target = np.array(target)\n",
    "route = pd.read_csv(path,header=None)\n",
    "route = np.array(route)\n",
    "for each in route:\n",
    "    from_ = each[0]\n",
    "    to_ = each[1]\n",
    "    weight = each[2]\n",
    "    k1 = member.index(from_)\n",
    "    k2 = member.index(to_)\n",
    "    target[k2][k1] = weight\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    输出每一列之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.0, 12.0, 76.0, 139.0, 43.0, 6.0, 193.0, 82.0, 1321.0, 9.0, 426.0, 58.0, 3.0, 34.0, 14.0, 1855.0, 12.0, 353.0, 1453.0, 434.0, 1296.0, 2.0, 111.0, 126.0, 19.0, 21.0, 608.0, 90.0, 13.0, 9.0, 158.0, 22.0, 3485.0, 166.0, 0.0, 53.0, 9939.0, 12.0, 0.0, 331.0, 89.0, 0.0, 0.0, 5.0, 741.0, 23.0, 195.0, 1263.0, 2.0, 23.0, 53.0, 34.0, 434.0, 261.0, 0.0, 0.0, 88.0, 96.0, 25.0, 64.0, 50.0, 58.0, 154.0, 155.0, 460.0, 24.0, 172.0, 564.0, 895.0, 205.0, 87.0, 891.0, 666.0, 2.0, 123.0, 173.0, 28.0, 0.0, 3.0, 75.0, 34.0, 9.0, 70.0, 1032.0, 503.0, 12.0, 96.0, 0.0, 629.0, 1172.0, 121.0, 14.0, 4.0, 107.0, 0.0, 577.0, 36.0, 843.0, 913.0, 99.0, 120.0, 628.0, 2576.0, 191.0, 18.0, 902.0, 4746.0, 0.0, 38.0, 27.0, 10.0, 160.0, 32.0, 5715.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "row_sum = list(target.sum(axis=0))\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    将每一列的和调整为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for col in range(n):\n",
    "    if row_sum[col] != 0:\n",
    "        for row in range(n):\n",
    "            target[row][col] = target[row][col]/row_sum[col]\n",
    "print(target.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    可以看到，在转移矩阵中，有若干列的和为0。也就是说，这个矩阵不是随机矩阵，不满足收敛的条件。为了让转移矩阵收敛，采用抽税法。\n",
    "$$v^{'}={\\beta}Mv+(1-{\\beta})e/n$$\n",
    "\n",
    "    从公式中可以看出，除了转移矩阵M，还需要一个向量e和阻尼系数，向量v的大小是n*1，其初始值全为1/n。\n",
    "    \n",
    "```python\n",
    "def pagerank(n,transmatrix):\n",
    "    #生成概率\n",
    "    beta = 0.8\n",
    "    x = np.ones((n,1))/n\n",
    "    p = np.ones((n,1))/n\n",
    "    for i in range(1000):\n",
    "        if x.all == (beta*np.dot(transmatrix, x)+(1-beta)*p).all:\n",
    "            print(i)\n",
    "            break\n",
    "        x = beta*np.dot(transmatrix, x)+(1-beta)*p\n",
    "        print(x.sum(axis = 0))\n",
    "    return x\n",
    "```\n",
    "    从理论上说，Pagerank算法迭代结束的条件是前后向量v不变，但是在实际操作中，v保持不变几乎是不可能的，而且在测试中，一般迭代20次左右，v的值就不会有太大的变化，因此就取这时候的v作为最终的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(n,transmatrix):\n",
    "    #生成概率\n",
    "    beta = 0.8\n",
    "    x = np.ones((n,1))/n\n",
    "    p = np.ones((n,1))/n\n",
    "    for i in range(30):\n",
    "        if x.all == (beta*np.dot(transmatrix, x)+(1-beta)*p).all:\n",
    "            print(i)\n",
    "            break\n",
    "        x = beta*np.dot(transmatrix, x)+(1-beta)*p\n",
    "        print(x.sum(axis = 0))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92347826]\n",
      "[0.90602698]\n",
      "[0.89399909]\n",
      "[0.8851917]\n",
      "[0.87822917]\n",
      "[0.87311115]\n",
      "[0.86921147]\n",
      "[0.86627304]\n",
      "[0.86402978]\n",
      "[0.86232016]\n",
      "[0.86100958]\n",
      "[0.86000361]\n",
      "[0.85922895]\n",
      "[0.85863136]\n",
      "[0.85816939]\n",
      "[0.85781168]\n",
      "[0.85753429]\n",
      "[0.85731891]\n",
      "[0.85715149]\n",
      "[0.85702122]\n",
      "[0.85691978]\n",
      "[0.85684072]\n",
      "[0.85677906]\n",
      "[0.85673096]\n",
      "[0.85669341]\n",
      "[0.85666408]\n",
      "[0.85664118]\n",
      "[0.85662327]\n",
      "[0.85660928]\n",
      "[0.85659834]\n"
     ]
    }
   ],
   "source": [
    "x = pagerank(n, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    接着按照（姓名-权重）的方式，输出每个高管（节点）对应的权值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrea Ring,0.002728605116871463\n",
      "Andrew Lewis,0.0033074412124809074\n",
      "Andy Zipper,0.007116659900528143\n",
      "Barry Tycholiz,0.008581661835954206\n",
      "Benjamin Rogers,0.0029937398996815693\n",
      "Bill Rapp,0.0017391304347826083\n",
      "Cara Semperger,0.008641095472192469\n",
      "Chris Dorland,0.0035509576084816943\n",
      "Chris Germany,0.008995687920789894\n",
      "Cooper Richey,0.002688318732481878\n",
      "Dan Hyvl,0.013226523760780632\n",
      "Dana Davis,0.010725000451209705\n",
      "Danny McCarty,0.003678327435693773\n",
      "Daren Farmer,0.003296468549645668\n",
      "Darrell Schoolcraft,0.0032723768162083483\n",
      "David Delainey,0.01355757515436347\n",
      "Diana Scholtes,0.006667184350229421\n",
      "Don Baughman,0.008766699634833373\n",
      "Drew Fossum,0.007386435088759931\n",
      "Elizabeth Sager,0.010725373077700193\n",
      "Eric Bass,0.007859846734066653\n",
      "Eric Saibi,0.0018684334525537261\n",
      "Errol McLaughlin,0.0038807748034928105\n",
      "Fletcher Sturm,0.004479017628270733\n",
      "Frank Ermis,0.007361172179368317\n",
      "Geir Solberg,0.002045029270676544\n",
      "Gerald Nemec,0.015163488299863219\n",
      "Greg Whalley,0.010344076112902586\n",
      "Greg Wolfe,0.006255152122237445\n",
      "Holden Salisbury,0.0027328626726777086\n",
      "Hunter Shively,0.0098964958106338\n",
      "James Derrick,0.0031605966320588617\n",
      "James Steffes,0.012917397771452604\n",
      "Jane Tholt,0.006410350317672451\n",
      "Jason Wolfe,0.0029293784990714372\n",
      "Jay Reitmeyer,0.006948701621921846\n",
      "Jeff Dasovich,0.015311239788195872\n",
      "Jeff King,0.005185793106620949\n",
      "Jeffery Skilling,0.004054441922891722\n",
      "Jeffrey Shankman,0.010147367529289674\n",
      "Jim Schwieger,0.004889633519541414\n",
      "Joe Parks,0.0018684334525537261\n",
      "Joe Quenet,0.0033650730982131825\n",
      "Joe Stepenovitch,0.004814504206500123\n",
      "John Arnold,0.012446911595159153\n",
      "John Forney,0.004432205820579312\n",
      "John Hodge,0.015053223991498419\n",
      "John Lavorato,0.022681562318962782\n",
      "John Zufferli,0.003089179726661885\n",
      "Juan Hernandez,0.0039133210392474395\n",
      "Judy Townsend,0.005799636807333387\n",
      "Kam Keiser,0.004547969434481185\n",
      "Kate Symes,0.013829081752611242\n",
      "Kay Mann,0.003451118549425593\n",
      "Keith Holst,0.0065031443131193\n",
      "Kenneth Lay,0.003261282740282449\n",
      "Kevin Hyatt,0.017867834625873356\n",
      "Kevin Presto,0.007942987510675684\n",
      "Kevin Ruscitti,0.005872702497110881\n",
      "Kim Ward,0.007481549359978753\n",
      "Kimberly Watson,0.0029326910589577942\n",
      "Larry Campbell,0.008540052879005989\n",
      "Lawrence May,0.0027835022958506888\n",
      "Lindy Donoho,0.008751432150400997\n",
      "Louise Kitchen,0.011956557369996417\n",
      "Lynn Blair,0.004594435535607991\n",
      "Marie Heard,0.006603274829772747\n",
      "Mark Haedicke,0.015343966080713272\n",
      "Mark Taylor,0.024324788567821724\n",
      "Mark Whitt,0.007037548695621151\n",
      "Martin Cuilla,0.004523962109651101\n",
      "Matthew Lenhart,0.013055383839558081\n",
      "Michael Grigsby,0.011333142861346807\n",
      "Michael Maggi,0.0046763070112000385\n",
      "Michelle Cash,0.004079031923480969\n",
      "Michelle Lokay,0.015665143949338715\n",
      "Mike Carson,0.0032513191029511406\n",
      "Mike Swerzbin,0.0039300606203648185\n",
      "Monika Causholli,0.004593764620149049\n",
      "Patrice Mims,0.004075377157145831\n",
      "Paul Lucci,0.005440465203765715\n",
      "Paul Thomas,0.002120210617131427\n",
      "Peter Keavey,0.0030652895564890186\n",
      "Philip Allen,0.0109609628687287\n",
      "Phillip Love,0.007491414946043958\n",
      "Phillip Platter,0.005412225118052893\n",
      "Randall Gay,0.00479610589636295\n",
      "Richard Ring,0.003139830294347704\n",
      "Richard Sanders,0.01415904057241656\n",
      "Richard Shapiro,0.013909533976371955\n",
      "Rick Buy,0.0067869748458964764\n",
      "Robert Badeer,0.005861694795296191\n",
      "Robert Benson,0.002447876098936296\n",
      "Rod Hayslett,0.011231114471214432\n",
      "Ryan Slinger,0.002537308068771301\n",
      "Sally Beck,0.005839496628955768\n",
      "Sandra Brawner,0.004333700205306527\n",
      "Scott Neal,0.006802555480552303\n",
      "Shelley Corman,0.007168059023045052\n",
      "Stacy Dickson,0.009745696940954071\n",
      "Stanley Horton,0.005799023591711867\n",
      "Stephanie Panus,0.0059646737628741425\n",
      "Steven Kean,0.016631131544517525\n",
      "Susan Bailey,0.008105223184416465\n",
      "Susan Pereira,0.002700511008425106\n",
      "Susan Scott,0.011630226497675781\n",
      "Tana Jones,0.024266125766639748\n",
      "Teb Lokey,0.002326254418307269\n",
      "Theresa Staab,0.0022525517073684964\n",
      "Thomas Martin,0.019202011182967698\n",
      "Tom Donohoe,0.0024745592423883224\n",
      "Tori Kuykendall,0.008493422385718898\n",
      "Tracy Geaccone,0.005604682050004279\n",
      "Vince Kaminski,0.020192509146343216\n",
      "Vladi Pimenov,0.0019459011264979596\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(member, x):\n",
    "    print(str(a)+','+str(b[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    在Gephi中，利用刚才获得的有向图边和之前的点数据构建有向图。\n",
    "![有向图](./pics/基础2.svg)\n",
    "\n",
    "    有向图的结论和无向图基本上是一致的，都可以看出几位特定的高管占据了网络的关键位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6人以群分——社团发现\n",
    "#### 利用Gephi进行社团发现\n",
    "    话不多说，直接上图\n",
    "##### 无向图的社团发现\n",
    "![社团发现](./pics/社团.svg)\n",
    "##### 有向图的社团发现\n",
    "![社团发现](./pics/有向社团.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7总结"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
